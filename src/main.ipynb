{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "# import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "folder=\"./dataset\"\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "data_train_val=torchvision.datasets.FashionMNIST(root=folder,train=True,download=True,transform=None)\n",
    "data_test=torchvision.datasets.FashionMNIST(root=folder,train=False,download=True,transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=data_test.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# figure = plt.figure(figsize = (8,8))\n",
    "# cols, rows = 4, 4\n",
    "# ids=torch.randint(len(data_test), size = (cols*rows,))\n",
    "# for i in range (1, cols*rows + 1):\n",
    "#     image, label = data_test[ids[i-1].item()]\n",
    "#     figure.add_subplot(rows, cols, i)\n",
    "#     plt.title(classes[label])\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(image.squeeze(), cmap='gray')\n",
    "#     # plt.imshow(image.squeeze())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_train_val.data[:2000].shape\n",
    "data_test.data.shape\n",
    "# type(data_train_val.data)\n",
    "# data_test.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (10 points)\n",
    "- (a) Data cleansing and transformation (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets, trans=None):\n",
    "        self.x = inputs\n",
    "        self.y = targets\n",
    "        self.trans=trans\n",
    "        \n",
    "        # # for loss function\n",
    "        # self.pos_indices = np.flatnonzero(targets==1)\n",
    "        # self.pos_index_map = {}\n",
    "        # for i, idx in enumerate(self.pos_indices):\n",
    "        #     self.pos_index_map[idx] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # self.mode = 'train' #TODO\n",
    "        # if self.mode == 'train':\n",
    "        #    idx2 = self.pos_index_map[idx] if idx in self.pos_indices else -1\n",
    "\n",
    "        if self.trans == None:\n",
    "            # return (idx2, self.x[idx], self.y[idx])\n",
    "            # print(\"no trans\")\n",
    "            return ( self.x[idx], self.y[idx])\n",
    "        else:\n",
    "            # return (idx2, self.trans(self.x[idx]), self.y[idx]) \n",
    "            # print(\"trans\")\n",
    "            return ( self.trans(self.x[idx]), self.y[idx]) \n",
    "\n",
    "def ds_trans(ds_input,trans_flag):\n",
    "    data_input=ds_input.data.clone().detach()\n",
    "    labels_input=ds_input.targets\n",
    "    if data_input.ndim <= 3:\n",
    "        # train_transform = transforms.Compose([\n",
    "        #     transforms.ToPILImage(),\n",
    "        #     transforms.Grayscale(3),\n",
    "        #     transforms.Resize((32, 32)), \n",
    "        #     transforms.ToTensor(),\n",
    "        #      transforms.Normalize(0.5, 0.5)\n",
    "        # ])\n",
    "        # train_transform = transforms.Compose([\n",
    "        #     transforms.Resize((224, 224))\n",
    "        # ])\n",
    "        transform_train_val=transforms.Resize((64, 64))\n",
    "        transform_test=transforms.Resize((64, 64))\n",
    "    elif data_input.ndim == 4:\n",
    "        # Should not come here\n",
    "        print(\"WRONG!\")\n",
    "\n",
    "    data_input = data_input/255.0\n",
    "    if torch.isnan(data_input).any():\n",
    "        print(\"have NaN or Inf\")\n",
    "    # data_input = data_input.clone().detach()\n",
    "    # data_input = torch.tensor(data_input, dtype=torch.float32)\n",
    "    data_input = data_input[:,None,:,:]\n",
    "    print(data_input.shape)\n",
    "    # flag_labels = torch.tensor(flag_labels) \n",
    "    # flag_labels = flag_labels[:,None]\n",
    "    # print(flag_labels.shape)\n",
    "    \n",
    "    if trans_flag==1:\n",
    "        flag_ds_new = dataset(data_input, labels_input, trans=transform_train_val)\n",
    "    elif trans_flag==2:\n",
    "        flag_ds_new = dataset(data_input, labels_input, trans=transform_test)\n",
    "    else:\n",
    "        flag_ds_new = dataset(data_input, labels_input, trans=None)\n",
    "    return (flag_ds_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "ds_processed_train_val=ds_trans(ds_input=data_train_val,trans_flag=1)\n",
    "ds_processed_test=ds_trans(ds_input=data_test,trans_flag=2)\n",
    "# ds_processed.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "ds_processed_train_split, ds_processed_val_split = torch.utils.data.random_split(data_train_val, [50000, 10000]) \n",
    "ds_processed_train_sampler=SubsetRandomSampler(ds_processed_train_split.indices)\n",
    "ds_processed_val_sampler=SubsetRandomSampler(ds_processed_val_split.indices)\n",
    "\n",
    "batch_size=64\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=ds_processed_train,\n",
    "#                             batch_size=batch_size,\n",
    "#                             shuffle=True)\n",
    "loader_train = torch.utils.data.DataLoader(ds_processed_train_val, batch_size=batch_size,sampler=ds_processed_train_sampler,\n",
    "                                            num_workers=2)\n",
    "loader_val = torch.utils.data.DataLoader(ds_processed_train_val, batch_size=batch_size,sampler=ds_processed_val_sampler,\n",
    "                                            num_workers=2)\n",
    "loader_test = torch.utils.data.DataLoader(ds_processed_test, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/percool/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/percool/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 64, 64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inputs, targets) in enumerate(loader_train):\n",
    "    break\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_317275/738004040.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp3=torch.tensor(temp,dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Dataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m temp4\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mtype\u001b[39m(data_train_val\u001b[39m.\u001b[39mtargets)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m ds\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset(temp4,data_train_val\u001b[39m.\u001b[39;49mtargets,transform\u001b[39m=\u001b[39;49mdata_transform)\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "# transform images size 28,28 to 224,224\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "# data_transform = transforms.Compose(\n",
    "#     [transforms.ToPILImage(),\n",
    "#     transforms.Resize((224, 224), interpolation=PIL.Image.NEAREST), \n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[.5], std=[.5])])\n",
    "data_transform = transforms.Compose(\n",
    "    [ \n",
    "    transforms.Resize((224, 224))])\n",
    "# data_train_val=torchvision.datasets.FashionMNIST(root=folder,train=True,download=True,transform=data_transform)\n",
    "# data_test=torchvision.datasets.FashionMNIST(root=folder,train=False,download=True,transform=data_transform)\n",
    "# data_transform(data_test.data)\n",
    "\n",
    "# reshape torch.Size([10000, 28, 28]) to torch.Size([10000,1, 28, 28])\n",
    "# data_transform=transforms.Resize((224, 224))\n",
    "\n",
    "# temp=data_transform(data_test.data)\n",
    "# data_test.data=temp\n",
    "# temp.shape\n",
    "\n",
    "# temp=data_test.data\n",
    "# transform(temp)\n",
    "temp= data_train_val.data[:, None, :, :]\n",
    "print(temp.shape)\n",
    "# data_transform = transforms.Compose(\n",
    "#     [ \n",
    "#     transforms.Resize((224, 224))])\n",
    "data_transform = transforms.Compose(\n",
    "    [ \n",
    "    transforms.ToPILImage()])\n",
    "temp2=data_transform(temp[1])\n",
    "type(temp2)\n",
    "# data_train_val.data=temp2\n",
    "\n",
    "# torch.utils.data.Dataset\n",
    "temp3=torch.tensor(temp,dtype=torch.float32)\n",
    "temp4=temp3/255\n",
    "temp4.shape\n",
    "type(data_train_val.targets)\n",
    "ds=torch.utils.data.Dataset(temp4,data_train_val.targets,transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (b) Data splitting (i.e., training, validation, and test splits) (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_train, set_val = torch.utils.data.random_split(data_train_val, [50000, 10000])\n",
    "len(set_train.indices)\n",
    "type(set_val)\n",
    "batch_size=64\n",
    "loader_train = torch.utils.data.DataLoader(set_train, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "loader_val = torch.utils.data.DataLoader(set_val, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "loader_test = torch.utils.data.DataLoader(data_test, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_val.data[1].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device ='cuda' if torch.cuda.is_available else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'wide_resnet50_2', pretrained=True)\n",
    "model=torchvision.models.vgg13()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.conv1=torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.features[0]=torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "model=model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainloop (dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        # Compute the error rate\n",
    "        prediction = model(X)\n",
    "        loss = loss_fn(prediction, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f'loss:{loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "            \n",
    "def evaluate (dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X,y = X.to(device),y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/percool/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/percool/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:6.921896 [    0/60000]\n",
      "loss:2.628282 [ 6400/60000]\n",
      "loss:2.230845 [12800/60000]\n",
      "loss:1.963102 [19200/60000]\n",
      "loss:1.536317 [25600/60000]\n",
      "loss:1.304635 [32000/60000]\n",
      "loss:0.947163 [38400/60000]\n",
      "loss:0.765277 [44800/60000]\n",
      "Validation Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/percool/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/home/percool/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 11.0%, Avg loss: 0.895487 \n",
      "\n",
      "Test Error\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m evaluate(loader_val, model, loss_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest Error\u001b[39m\u001b[39m\"\u001b[39m);\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m evaluate(loader_test, model, loss_fn)\n",
      "\u001b[1;32m/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m X,y \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     X,y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device),y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_fn(pred, y)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/percool/Class/ECEN758/ECEN758_Proj/src/main.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/fashion/lib/python3.10/site-packages/torch/nn/functional.py:791\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 791\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    trainloop(loader_train, model, loss_fn, optimizer)\n",
    "    print(\"Validation Error\");\n",
    "    evaluate(loader_val, model, loss_fn)\n",
    "    print(\"Test Error\");\n",
    "    evaluate(loader_test, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
